{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b463943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Mean Absolute Percentage Error</th>\n",
       "      <th>Normalized Mean Squared Error</th>\n",
       "      <th>R^2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>70.083731</td>\n",
       "      <td>7611.219923</td>\n",
       "      <td>0.657789</td>\n",
       "      <td>0.077788</td>\n",
       "      <td>0.922212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>58.518000</td>\n",
       "      <td>8443.495887</td>\n",
       "      <td>0.257034</td>\n",
       "      <td>0.086294</td>\n",
       "      <td>0.913706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP Regressor</td>\n",
       "      <td>83.241591</td>\n",
       "      <td>13518.711214</td>\n",
       "      <td>0.243468</td>\n",
       "      <td>0.138164</td>\n",
       "      <td>0.861836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVR</td>\n",
       "      <td>235.431720</td>\n",
       "      <td>94979.271884</td>\n",
       "      <td>3.728616</td>\n",
       "      <td>0.970705</td>\n",
       "      <td>0.029295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SMO with Sigmoid Kernel</td>\n",
       "      <td>227.201465</td>\n",
       "      <td>88278.464248</td>\n",
       "      <td>3.622396</td>\n",
       "      <td>0.902221</td>\n",
       "      <td>0.097779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SMO with polynomial Kernel</td>\n",
       "      <td>161.270747</td>\n",
       "      <td>34556.725266</td>\n",
       "      <td>2.677129</td>\n",
       "      <td>0.353176</td>\n",
       "      <td>0.646824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMO with RBF Kernel</td>\n",
       "      <td>235.167226</td>\n",
       "      <td>94876.092943</td>\n",
       "      <td>3.721497</td>\n",
       "      <td>0.969650</td>\n",
       "      <td>0.030350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Mean Absolute Error  Mean Squared Error  \\\n",
       "0           Linear Regression            70.083731         7611.219923   \n",
       "1     Decision Tree Regressor            58.518000         8443.495887   \n",
       "2               MLP Regressor            83.241591        13518.711214   \n",
       "3                         SVR           235.431720        94979.271884   \n",
       "4     SMO with Sigmoid Kernel           227.201465        88278.464248   \n",
       "5  SMO with polynomial Kernel           161.270747        34556.725266   \n",
       "6         SMO with RBF Kernel           235.167226        94876.092943   \n",
       "\n",
       "   Mean Absolute Percentage Error  Normalized Mean Squared Error  R^2 Score  \n",
       "0                        0.657789                       0.077788   0.922212  \n",
       "1                        0.257034                       0.086294   0.913706  \n",
       "2                        0.243468                       0.138164   0.861836  \n",
       "3                        3.728616                       0.970705   0.029295  \n",
       "4                        3.622396                       0.902221   0.097779  \n",
       "5                        2.677129                       0.353176   0.646824  \n",
       "6                        3.721497                       0.969650   0.030350  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Your existing code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "df = pd.read_csv('Effort estimation data set.csv')\n",
    "\n",
    "X = df.drop(columns=['Effort (Actual)'])\n",
    "y = df['Effort (Actual)']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "\n",
    "mlp = MLPRegressor()\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svr_sigmoid = SVR(kernel='sigmoid')\n",
    "svr_sigmoid.fit(X_train_scaled, y_train)\n",
    "svr_sigmoid_pred = svr_sigmoid.predict(X_test_scaled)\n",
    "\n",
    "svr_poly = SVR(kernel='poly')\n",
    "svr_poly.fit(X_train_scaled, y_train)\n",
    "svr_poly_pred = svr_poly.predict(X_test_scaled)\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "svr_rbf.fit(X_train_scaled, y_train)\n",
    "svr_rbf_pred = svr_rbf.predict(X_test_scaled)\n",
    "\n",
    "def normalized_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred) / np.var(y_true)\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    nmse = normalized_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, mape, nmse, r2\n",
    "\n",
    "models = [lr, dt, mlp, svr, svr_sigmoid, svr_poly, svr_rbf]\n",
    "predictions = [lr_pred, dt_pred, mlp_pred, svr_pred, svr_sigmoid_pred, svr_poly_pred, svr_rbf_pred]\n",
    "model_names = ['Linear Regression', 'Decision Tree Regressor', 'MLP Regressor', 'SVR', 'SMO with Sigmoid Kernel', 'SMO with polynomial Kernel', 'SMO with RBF Kernel']\n",
    "\n",
    "results = []\n",
    "for model, pred, name in zip(models, predictions, model_names):\n",
    "    mae, mse, mape, nmse, r2 = evaluate_performance(y_test, pred)\n",
    "    results.append([name, mae, mse, mape, nmse, r2])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Mean Absolute Error\", \"Mean Squared Error\", \"Mean Absolute Percentage Error\", \"Normalized Mean Squared Error\", \"R^2 Score\"])\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f75e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear Regression\n",
      "  copy_X: True\n",
      "  fit_intercept: True\n",
      "  n_jobs: None\n",
      "  positive: False\n",
      "\n",
      "Model: Decision Tree Regressor\n",
      "  ccp_alpha: 0.0\n",
      "  criterion: squared_error\n",
      "  max_depth: None\n",
      "  max_features: None\n",
      "  max_leaf_nodes: None\n",
      "  min_impurity_decrease: 0.0\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 2\n",
      "  min_weight_fraction_leaf: 0.0\n",
      "  random_state: None\n",
      "  splitter: best\n",
      "\n",
      "Model: MLP Regressor\n",
      "  activation: relu\n",
      "  alpha: 0.0001\n",
      "  batch_size: auto\n",
      "  beta_1: 0.9\n",
      "  beta_2: 0.999\n",
      "  early_stopping: False\n",
      "  epsilon: 1e-08\n",
      "  hidden_layer_sizes: (100,)\n",
      "  learning_rate: constant\n",
      "  learning_rate_init: 0.001\n",
      "  max_fun: 15000\n",
      "  max_iter: 200\n",
      "  momentum: 0.9\n",
      "  n_iter_no_change: 10\n",
      "  nesterovs_momentum: True\n",
      "  power_t: 0.5\n",
      "  random_state: None\n",
      "  shuffle: True\n",
      "  solver: adam\n",
      "  tol: 0.0001\n",
      "  validation_fraction: 0.1\n",
      "  verbose: False\n",
      "  warm_start: False\n",
      "\n",
      "Model: SVR\n",
      "  C: 1.0\n",
      "  cache_size: 200\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  epsilon: 0.1\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "  max_iter: -1\n",
      "  shrinking: True\n",
      "  tol: 0.001\n",
      "  verbose: False\n",
      "\n",
      "Model: SMO with Sigmoid Kernel\n",
      "  C: 1.0\n",
      "  cache_size: 200\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  epsilon: 0.1\n",
      "  gamma: scale\n",
      "  kernel: sigmoid\n",
      "  max_iter: -1\n",
      "  shrinking: True\n",
      "  tol: 0.001\n",
      "  verbose: False\n",
      "\n",
      "Model: SMO with polynomial Kernel\n",
      "  C: 1.0\n",
      "  cache_size: 200\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  epsilon: 0.1\n",
      "  gamma: scale\n",
      "  kernel: poly\n",
      "  max_iter: -1\n",
      "  shrinking: True\n",
      "  tol: 0.001\n",
      "  verbose: False\n",
      "\n",
      "Model: SMO with RBF Kernel\n",
      "  C: 1.0\n",
      "  cache_size: 200\n",
      "  coef0: 0.0\n",
      "  degree: 3\n",
      "  epsilon: 0.1\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "  max_iter: -1\n",
      "  shrinking: True\n",
      "  tol: 0.001\n",
      "  verbose: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the parameters and default values for each model\n",
    "model_params = {\n",
    "    'Linear Regression': lr.get_params(),\n",
    "    'Decision Tree Regressor': dt.get_params(),\n",
    "    'MLP Regressor': mlp.get_params(),\n",
    "    'SVR': svr.get_params(),\n",
    "    'SMO with Sigmoid Kernel': svr_sigmoid.get_params(),\n",
    "    'SMO with polynomial Kernel': svr_poly.get_params(),\n",
    "    'SMO with RBF Kernel': svr_rbf.get_params(),\n",
    "}\n",
    "\n",
    "# Display the parameters and default values for each model\n",
    "for model_name, params in model_params.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for param_name, default_value in params.items():\n",
    "        print(f\"  {param_name}: {default_value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17128047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear Regression\n",
      "  Best Parameters:\n",
      "    copy_X: True\n",
      "    fit_intercept: True\n",
      "    positive: False\n",
      "  Normalized Mean Squared Error: 0.07778798316065162\n",
      "  Mean Absolute Error: 70.08373079843935\n",
      "  Mean Squared Error: 7611.219922889723\n",
      "  Mean Absolute Percentage Error: 0.6577887118231351\n",
      "  R^2 Score: 0.9222120168393484\n"
     ]
    }
   ],
   "source": [
    "# LINEAR REGRESSION\n",
    "\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid_lr = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'positive': [True, False]\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Linear Regression\n",
    "grid_search_lr = GridSearchCV(LinearRegression(), param_grid_lr, cv=8, scoring='neg_mean_squared_error')\n",
    "grid_search_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search\n",
    "best_params_grid_lr = grid_search_lr.best_params_\n",
    "best_score_grid_lr = grid_search_lr.best_score_\n",
    "\n",
    "# Train Linear Regression with the best parameters from Grid Search\n",
    "best_lr_grid = LinearRegression(**best_params_grid_lr)\n",
    "best_lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best Linear Regression model from Grid Search\n",
    "best_lr_pred_grid = best_lr_grid.predict(X_test_scaled)\n",
    "\n",
    "# Define a function to evaluate performance metrics\n",
    "def normalized_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred) / np.var(y_true)\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    nmse = normalized_mean_squared_error(y_true, y_pred)\n",
    "    return mae, mse, mape, r2, nmse\n",
    "\n",
    "# Evaluate performance metrics for the best Linear Regression model from Grid Search\n",
    "mae_grid, mse_grid, mape_grid, r2_grid, nmse_grid = evaluate_performance(y_test, best_lr_pred_grid)\n",
    "\n",
    "# Print the model name\n",
    "print(\"Model: Linear Regression\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_lr.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid}\")\n",
    "print(f\"  R^2 Score: {r2_grid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df23328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the parameter grid for Grid Search for Decision Tree Regressor with a wider range of values\n",
    "param_grid_dt = {\n",
    "    'max_depth': range(1, 10),  # Set a wider range from 1 to 20 for max_depth\n",
    "    'max_leaf_nodes': range(2, 10),  # Set a wider range from 2 to 100 for max_leaf_nodes\n",
    "    'min_samples_leaf': range(1, 6),  # Set a wider range from 1 to 10 for min_samples_leaf\n",
    "    'min_samples_split': range(2, 6),  # Set a wider range from 2 to 20 for min_samples_split\n",
    "    \"criterion\": [\"poisson\", \"absolute_error\", \"squared_error\", \"friedman_mse\"]\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Decision Tree Regressor\n",
    "grid_search_dt = GridSearchCV(DecisionTreeRegressor(), param_grid_dt, cv=8, scoring='neg_mean_squared_error')\n",
    "grid_search_dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search\n",
    "best_params_grid_dt = grid_search_dt.best_params_\n",
    "best_score_grid_dt = grid_search_dt.best_score_\n",
    "\n",
    "# Train Decision Tree Regressor with the best parameters from Grid Search\n",
    "best_dt_grid = DecisionTreeRegressor(**best_params_grid_dt)\n",
    "best_dt_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best Decision Tree Regressor model from Grid Search\n",
    "best_dt_pred_grid = best_dt_grid.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance metrics for the best Linear Regression model from Grid Search\n",
    "mae_grid, mse_grid, mape_grid, r2_grid, nmse_grid = evaluate_performance(y_test, best_dt_pred_grid)\n",
    "\n",
    "# Print the model name\n",
    "print(\"Model: Decision Tree Regressor\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_dt.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid}\")\n",
    "print(f\"  R^2 Score: {r2_grid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7853ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in square\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in matmul\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in matmul\")\n",
    "\n",
    "# Define the parameter grid for Grid Search for MLP Regressor\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 20)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Perform Grid Search for MLP Regressor\n",
    "grid_search_mlp = GridSearchCV(MLPRegressor(), param_grid_mlp, cv=8, scoring='neg_mean_squared_error')\n",
    "grid_search_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search\n",
    "best_params_grid_mlp = grid_search_mlp.best_params_\n",
    "best_score_grid_mlp = grid_search_mlp.best_score_\n",
    "\n",
    "# Train MLP Regressor with the best parameters from Grid Search\n",
    "best_mlp_grid = MLPRegressor(**best_params_grid_mlp)\n",
    "best_mlp_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best MLP Regressor model from Grid Search\n",
    "best_mlp_pred_grid = best_mlp_grid.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance metrics for the best MLP Regressor model from Grid Search\n",
    "mae_grid_mlp, mse_grid_mlp, mape_grid_mlp, r2_grid_mlp, nmse_grid_mlp = evaluate_performance(y_test, best_mlp_pred_grid)\n",
    "# Print the model name\n",
    "print(\"Model: MLP Regressor\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_mlp.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid_mlp}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid_mlp}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid_mlp}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid_mlp}\")\n",
    "print(f\"  R^2 Score: {r2_grid_mlp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in square\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in matmul\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in matmul\")\n",
    "\n",
    "\n",
    "# Define the parameter grid for Grid Search for SVR\n",
    "param_grid_svr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Expanded range for C\n",
    "    'epsilon': [0.001, 0.01, 0.1, 0.5, 1.0],  # Expanded range for epsilon\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Include 'poly' kernel\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0],  # Expanded range for gamma and additional values\n",
    "    'degree': [2, 3, 4],  # Include 'degree' parameter for polynomial kernel\n",
    "    'coef0': [0.0, 0.1, 0.5, 1.0],  # Include 'coef0' parameter for polynomial and sigmoid kernels\n",
    "}\n",
    "\n",
    "# Perform Grid Search for SVR\n",
    "grid_search_svr = GridSearchCV(SVR(), param_grid_svr, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search with SVR\n",
    "best_params_grid_svr = grid_search_svr.best_params_\n",
    "best_score_grid_svr = grid_search_svr.best_score_\n",
    "\n",
    "# Train SVR with the best parameters from Grid Search\n",
    "best_svr_grid = SVR(**best_params_grid_svr)\n",
    "best_svr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best SVR model from Grid Search\n",
    "best_svr_pred_grid = best_svr_grid.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance metrics for the best SVR model from Grid Search\n",
    "mae_grid_svr, mse_grid_svr, mape_grid_svr, r2_grid_svr, nmse_grid_svr = evaluate_performance(y_test, best_svr_pred_grid)\n",
    "\n",
    "# Print the model name\n",
    "print(\"Model: SVR\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_svr.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics including NMSE\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid_svr}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid_svr}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid_svr}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid_svr}\")\n",
    "print(f\"  R^2 Score: {r2_grid_svr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMO WITH SIGMOID KERNEL\n",
    "\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the parameter grid for Grid Search for SMO with Sigmoid Kernel\n",
    "param_grid_smo_sigmoid = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "    'coef0': [0.0, 0.1, 0.5],  # Constant term of the sigmoid kernel\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'tol': [0.0001, 0.001, 0.01],  # Tolerance for stopping criterion\n",
    "    'max_iter': [-1, 100, 1000]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Perform Grid Search for SMO with Sigmoid Kernel\n",
    "grid_search_smo_sigmoid = GridSearchCV(SVR(kernel='sigmoid'), param_grid_smo_sigmoid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_smo_sigmoid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search with SMO with Sigmoid Kernel\n",
    "best_params_grid_smo_sigmoid = grid_search_smo_sigmoid.best_params_\n",
    "best_score_grid_smo_sigmoid = grid_search_smo_sigmoid.best_score_\n",
    "\n",
    "# Train SMO with Sigmoid Kernel with the best parameters from Grid Search\n",
    "best_smo_sigmoid_grid = SVR(kernel='sigmoid', **best_params_grid_smo_sigmoid)\n",
    "best_smo_sigmoid_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best SMO with Sigmoid Kernel model from Grid Search\n",
    "best_smo_sigmoid_pred_grid = best_smo_sigmoid_grid.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance metrics for the best SMO with Sigmoid Kernel model from Grid Search\n",
    "mae_grid_smo_sigmoid, mse_grid_smo_sigmoid, mape_grid_smo_sigmoid, r2_grid_smo_sigmoid, nmse_grid_smo_sigmoid = evaluate_performance(y_test, best_smo_sigmoid_pred_grid)\n",
    "\n",
    "# Print the model name\n",
    "print(\"Model: SMO with Sigmoid Kernel\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_smo_sigmoid.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics including NMSE\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid_smo_sigmoid}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid_smo_sigmoid}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid_smo_sigmoid}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid_smo_sigmoid}\")\n",
    "print(f\"  R^2 Score: {r2_grid_smo_sigmoid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMO WITH POLYNOMIAL KERNEL\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the parameter grid for Grid Search for SMO with Polynomial Kernel\n",
    "param_grid_smo_poly = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "    'coef0': [0.0, 0.1, 0.5],  # Constant term of the polynomial kernel\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel function\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'tol': [0.0001, 0.001, 0.01],  # Tolerance for stopping criterion\n",
    "    'max_iter': [-1, 100, 1000]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Perform Grid Search for SMO with Polynomial Kernel\n",
    "grid_search_smo_poly = GridSearchCV(SVR(kernel='poly'), param_grid_smo_poly, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_smo_poly.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search with SMO with Polynomial Kernel\n",
    "best_params_grid_smo_poly = grid_search_smo_poly.best_params_\n",
    "best_score_grid_smo_poly = grid_search_smo_poly.best_score_\n",
    "\n",
    "# Train SMO with Polynomial Kernel with the best parameters from Grid Search\n",
    "best_smo_poly_grid = SVR(kernel='poly', **best_params_grid_smo_poly)\n",
    "best_smo_poly_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best SMO with Polynomial Kernel model from Grid Search\n",
    "best_smo_poly_pred_grid = best_smo_poly_grid.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance metrics for the best SMO with Polynomial Kernel model from Grid Search\n",
    "mae_grid_smo_poly, mse_grid_smo_poly, mape_grid_smo_poly, r2_grid_smo_poly, nmse_grid_smo_poly = evaluate_performance(y_test, best_smo_poly_pred_grid)\n",
    "\n",
    "# Print the model name\n",
    "print(\"Model: SMO with Polynomial Kernel\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_smo_poly.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics including NMSE\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid_smo_poly}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid_smo_poly}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid_smo_poly}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid_smo_poly}\")\n",
    "print(f\"  R^2 Score: {r2_grid_smo_poly}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a555525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMO WITH RBF KERNEL\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The objective has been evaluated at this point before.\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the parameter grid for Grid Search for SMO with RBF Kernel\n",
    "param_grid_smo_rbf = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'epsilon': [0.01, 0.1, 0.5],  # Epsilon in the epsilon-SVR model\n",
    "    'tol': [0.0001, 0.001, 0.01],  # Tolerance for stopping criterion\n",
    "    'max_iter': [-1, 100, 1000]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Perform Grid Search for SMO with RBF Kernel\n",
    "grid_search_smo_rbf = GridSearchCV(SVR(kernel='rbf'), param_grid_smo_rbf, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_smo_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score for Grid Search with SMO with RBF Kernel\n",
    "best_params_grid_smo_rbf = grid_search_smo_rbf.best_params_\n",
    "best_score_grid_smo_rbf = grid_search_smo_rbf.best_score_\n",
    "\n",
    "# Train SMO with RBF Kernel with the best parameters from Grid Search\n",
    "best_smo_rbf_grid = SVR(kernel='rbf', **best_params_grid_smo_rbf)\n",
    "best_smo_rbf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best SMO with RBF Kernel model from Grid Search\n",
    "best_smo_rbf_pred_grid = best_smo_rbf_grid.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance metrics for the best SMO with RBF Kernel model from Grid Search\n",
    "mae_grid_smo_rbf, mse_grid_smo_rbf, mape_grid_smo_rbf, r2_grid_smo_rbf, nmse_grid_smo_rbf = evaluate_performance(y_test, best_smo_rbf_pred_grid)\n",
    "\n",
    "# Print the model name\n",
    "print(\"Model: SMO with RBF Kernel\")\n",
    "\n",
    "# Print the best parameters set by grid search\n",
    "print(\"  Best Parameters:\")\n",
    "for param, value in best_params_grid_smo_rbf.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "\n",
    "# Print the performance metrics including NMSE\n",
    "print(f\"  Normalized Mean Squared Error: {nmse_grid_smo_rbf}\")\n",
    "print(f\"  Mean Absolute Error: {mae_grid_smo_rbf}\")\n",
    "print(f\"  Mean Squared Error: {mse_grid_smo_rbf}\")\n",
    "print(f\"  Mean Absolute Percentage Error: {mape_grid_smo_rbf}\")\n",
    "print(f\"  R^2 Score: {r2_grid_smo_rbf}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
